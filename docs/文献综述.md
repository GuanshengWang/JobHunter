#文献综述

##摘要&关键词

###摘要
随着互联网技术的高速发展，它已经越来越紧密的融入到人们日常生活的方法面面。作为人们获取外界信息的重要渠道，互联网上的信息资源正在呈爆炸式趋势增长。为了满足人们获取想要了解的信息的需求，一个个搜索引擎应运而生。但是搜索引擎无法很好的满足用户对于特定信息的精准度，密集度的需求，于是以主题网络爬虫为代表的网络爬虫技术慢慢的走进了越来越多人的视野。

本文对网络爬虫的概念，分类，研究现状以及相关技术等做出了回顾和总结，能让人们对网络爬虫技术有一个更清晰的认知和理解。其中还重点介绍了爬虫技术典型的几种搜索策略、以Jsoup为代表的java应用领域里的爬虫技术框架，为个性化爬虫系统的实现提供了思路。

###关键词
网络爬虫，精准，搜索策略，Jsoup


##英文版

###摘要
 Abstract:With the rapid development of Internet technology, it has become increasingly tightly integrated into people's daily life.As the important channel of people getting outside  information, data resources on the Internet is exploding.In order to satisfy people's demand for access to information, the search engine came into being.
But when Faced with the demand of  increasing accuracy,  intensive of specific information , search engine appeared to be inadequate.So web crawler technology slowly go into the vision of more and more people.
This paper made review and summary on the concept,classification,current research and related technology of Web crawler,which made people to have a clearer awareness and understanding about Crawler technology.It also highlights several typical search strategys of crawler technology ,and Jsoup, a crawler development framework, providing a guideline to achieve personalized crawler system 




###关键词
Web Crawler, accurancy ,Search strategy, Jsoup


##引言
伴随着互联网时代的到来，互联网技术的发展越来越快，互联网上的信息资源可谓是呈爆炸式趋势增长。为了满足人们获取想要了解的信息的需求，一个个搜索引擎应运而生，为大众所熟知的搜索引擎有Google,百度，Bing等，这些搜索引擎让人们的生活发生了翻天覆地的变化，如今人们几乎可以做到足不出户就获取到方方面面想要的信息，为个人的学习，生活，工作，娱乐，出行等带来了很大的便利。然而在海量的数据面前，搜索引擎能够提供的帮助也是有限的。在大数据时代，用户对特定信息的准确度，精确度以及密集度产生了越来越强烈的需求，这使得深度定制的网路爬虫技术有了大显身手的舞台，并越来越被人们所熟知和运用。


##网络爬虫概念及分类

###概念
网络爬虫(Web Crawler)，又称为网络蜘蛛(Web Spider),是搜索引擎的基础和重要组成部分，其本质上是一段自动下载网页内容的计算机程序或自动化脚本。网络爬虫通常从一个被称为Seeds(种子集)的URL集合开始运行，首先将这些URL全部放入到一个有序的待爬取队列中，按照一定的顺序从种子集中取出URL，并下载该URL所指向的页面。对所下载的页面进行分析之后，会提取出新的URL存入到待爬取的URL队列中。如此往复循环之前的过程，直到待爬取的URL队列为空，或者满足了某个特定的条件，爬虫程序才会终止，从而完成一次遍历Web的工作。这个过程被称为网络爬行。


###网络爬虫的分类

####通用网络爬虫
通用搜索引擎面向的对象是不同的群体,为了符合大众群体的需求,它必须 能够做到包罗万象,因此通用爬虫的最大心愿就是抓取所有的相关页面。通用爬 虫一般都是从种子页面开始,先将该页面中的内容下载到本地数据库中,然后用 网页链接提取工具从刚才下载的网页中提取链接,最后将提取出来的链接加入到 待爬取队列中。除此之外,通用爬虫在爬取信息资源的时候,都会对其重要程度 进行评价估计,以此来决定爬取信息的优先级顺序,这个重要程度的评价主要取决于网页访问情况、网页的质量以及链接关系结构等。

通用网络爬虫的基本工作流程如下[23]:(1) 首先选取一部分精心挑选的种子 URL;(2) 将这些 URL 放入待抓取队列;(3) 从待抓取队列中逐个取出 URL,并解析出其 DNS,得到对应的主机 IP,并将该 URL 对应的网页内容下载到本地数据库中,最后将这些下载下来的 URL 放进已抓取队列;(4) 对已抓取队列中的 URL 进行逐个解析,获取其对应页面中的其他链接 的 URL,最后将这些 URL 再次放入待抓取队列中,然后进入下一循环;(5) 直到满足一定的终止条件爬行结束。从上述的过程中可以看出:通用爬虫的最大目标是对整个互联网的进行遍 历。其遍历的算法多数采用经典的广度优先算法或者深度优先算法
通用网络爬虫的不足(1) 搜索精度问题。精度和广度是两个互相制约的概念,通用爬虫的目的是尽量搜索更多页面,其搜索精度必然会很低,而随着网络信息技术的高速发展,  网络信息急剧增加,通用爬虫搜索到的页面也会越来越多,其搜索精度问题会变 得越来越严重。(2) 搜索覆盖问题。虽然通用爬虫的最大目标是对整个网络进行遍历爬取, 但是由于网络资源数量十分巨大,全球数以亿计的服务器节点中存储着数量规模 庞大的信息系统,而搜索引擎的实际覆盖率却远低于预期水准。经权威机构研究 表明,即使是拥有数十亿索引规模的超大搜索引擎,其实际覆盖率也没有达到网 络信息的 80%。(3) 个性化缺失问题。由于通用爬虫对整个互联网的遍历是随机进行的,它 最大的任务就是把所有包含关键字的网页都呈现给用户,因此通用搜索引擎往往 只能满足群体大众的需求,而不能满足更加专业化、个性化用户的需求。(4) 效率问题。由于通用爬虫关注的页面非常多,其搜索效率必然不会很高, 虽然这可以通过硬件方面来弥补,但是对于可怜的用户来说,他们关注的页面可 能只有区区几个,而通用爬虫却把大量的页面都呈现给他们,而这都需要用户自 己一一去判断,这无疑是对用户自身体力的挑战。对于通用爬虫的种种不足[26],主题爬虫都能够一一解决,它是在通用爬虫的基础上细化而产生的一种新型爬虫,不但能快速抓取特定信息,还能够满足更加专业化、个性化用户的需求,此外,由于它关注的页面较少因此效率也较高。

####主体网络爬虫
主题网络爬虫在爬取信息时,并不面向整个 Web,而是面向确定的主题或 领域,过滤掉与主题无关的信息,使爬虫所获取的资源只与确定主题相关,只对 与主题相关的信息进行处理,建立索引用户接口,为特定用户构建特定主题的搜 索引擎[28]。主题网络爬虫并不像通用网络爬虫那样极大地追求覆盖率,而是将 抓取目标定为抓取特定主题信息,只为对该主题感兴趣的用户提供抓取服务。主 题网络爬虫用特定的搜索策略对特定信息进行抓取,之后并重复上一过程,一直 循环,直到满足终止条件时停止抓取。


相比较于通用网络爬虫来说,主题网络爬虫的工作流程更加复杂化,它需 用网页分析算法对过与主题无关的链接进行滤掉,而只留下与主题相关的链 接,并将其放入 URL 队列中等待下次抓取,接着再用某种搜索策略从 URL 队列 中选取下一步抓取的网页 URL,这样一直重复上述过程,直到满足系统的停止 条件才停止操作。相对于传统的网络爬虫,主题爬虫需要解决以下两个关键问题[30]:(1) 对网页数据的分析和过滤(即判断网页内容和网页链接是否与主题相 关);(2) 对网页进行搜索时的策略(即决定待爬行 URL 的访问次序)

通用网络爬虫的一般过程是:首先从种子页面开始,将该页面的内容抓取 到本地数据库中,然后对每一个链接进行逐个分析,最终将这些已经分析过的 链接加入到待抓取队列中。此外,它对互联网的遍历采用的是经典的广度优先 算法和者深度优先算法两种,而这些算法都是控制爬虫随机地对整个互联网进 行搜索抓取的。主题网络爬虫面向的是某个确定的主题,并以此来过滤掉与主题无关的页 面 URL,使爬虫所抓取的信息都与特定主题相关,然后才对采集到的主题相关 信息进行分析处理。主题爬虫主要解决的问题是在下载网页之前就要预测给定 网页与主题的相关程度,其算法主要是基于网页内容的分析和基于网页拓扑结 构的算法两大类。综上所述,主题网络爬虫显得更加“专”、“快”、“深”,其中的“专”指的是 主题网络爬虫在抓取信息是更有目的性和针对性;“快”指的是主题网络爬虫的 抓取效率更高,用户搜索时更加迅速便捷;“深”指的是主题网络爬虫从种子页 面开始抓取之后,它的入链程度更深[31]


###爬虫的URL搜索策略问题

按照网络爬虫对Web页面的抓取策略,总体来讲网络爬虫可以分为三类:广 度优先搜索策略爬虫、最佳优先搜索策略爬虫、深度优先搜索策略爬虫。

(1)广度优先搜索策略爬虫 广度优先搜索策略爬虫的主要原理是在抓取过程中,只有对当前层级范围内整个URL集合都完成遍历搜索之后,才开始进行下一层级的搜索。这种算法的设 计和实现都相对比较简单。在需要尽可能广泛地采集网页的情况下,执行广度优 先搜索策略的爬虫通常作为首选。目前许多对聚焦爬虫的研究都采用广度优先搜 索策略,这里存在的一个基本假设是,与种子URL在较短的链接距离内的网页之 间具有关联性的概率更高,因而其所对应的内容也就与主题具有较高的相关性。 另外一种常见的方法整合了广度优先搜索策略与网页过滤技术,首先按照广度优 先策略遍历网页,然后凭借过滤技术从中剔除无关的网页。但是随着抓取网页数 量规模的不断扩大,这种方式暴露出的缺陷是:程序需要处理和过滤大量无关的网页,算法效率因而受影响将变低。 
(2)最佳优先搜索策略爬虫最佳优先搜索策略爬虫的工作原理是在抓取Web页面的同时,对Web页面内 容进行分析,以计算目标URL对应的W曲页面与种子URL对应的Web页面之间的相关度,从中选取相关度最高的一个或多个URL进行抓取。换句话说,最佳搜 索策略爬虫在抓取页面时只会挑选那些最“有用"的Web页面进行抓取,而忽略 那些“没用”的Web页面。显然采用这种策略使得爬虫在开展抓取任务时可以比 较有针对性地获取页面,回避大量的无关页面,极大地提高了爬虫的抓取效率, 但是这种方式也存在着问题,即如何实现准确判断Web页面之间的相关性也是一 个难点,目前采用这一策略的爬虫这一环节或多或少存在一些缺陷,因而往往会 导致相当一部分“有用”的Web页面也会被爬虫所忽略。
(3)深度优先搜索策略爬虫 深度优先搜索策略爬虫的原理是从给定的初始网页开始,从中选择某条URL进入下一张页面,进而分析该网页中包含的URL,选择一条再进入,如此循环不 断地进行下去,直到完整地走完这一条URL线路之后再开始新的线路。深度优先 策略爬虫的设计和实现同样较为简单,但是这种页面抓取策略存在较大的缺陷, 例如一般门户网站提供的链接往往最具价值,但往下每深入一层,其网页价值也 将随之递减,采用深度优先的爬虫往往先是抓取了较多低价值的页面,而对重要 页面的获取比较滞后,也就是说抓取深度直接影响了抓取命中率以及抓取效率。 另一方面,网络上存在的“链接黑洞”(页面间的互相链接近乎无止境)会使深度 优先策略的爬虫陷入,这同样影响爬虫抓取的效率。因而相对于上述两种策略而 言,采用深度优先策略的爬虫比较少。按照网络爬虫的工作方式可以将网络爬虫分为两类:集中式网络爬虫和分布 式网络爬虫。
###集中式与分布式
按照网络爬虫的工作方式可以将网络爬虫分为两类:集中式网络爬虫和分布 式网络爬虫[27】。(1)集中式网络爬虫集中式网络爬虫的工作原理是:首先通过初始设定的种子URL爬取其所对应 的Web页面,通过爬虫的解析模块从Web页面中提取内容以及新的URL链接,保存新获取的URL链接到指定URL集合中,以进行后续抓取任务。其突出特点是运行在某台主机之上,从而开始访问互联网进行网页抓取任务。早期实现的网 络爬虫大多采用这种工作方式,但是如今,随着互联网规模的急剧膨胀,集中式 的网络爬虫往往受限于软硬件资源的限制,其采集效率往往难以满足业务的需求。(2)分布式网络爬虫 分布式网络爬虫的工作运行原理实际上可以看作是多个集中式爬虫的协同合作,进而弥补了集中式网络爬虫所存在的缺陷。分布式网络爬虫包含多个子网络爬 虫,其分布性体现在这些爬虫可以分布在不同的地理位置,每个子爬虫在执行抓取 任务的过程中与单个集中式爬虫基本一致,同样是从互联网上下载Web页面,并将 网页数据保存到本地文件系统中,从页面中解析出新的URL,并按照这些URL的指向继续爬行,从而不断抓取页面。分布式爬虫需要额外处理的是分割下载任务, 并将分割的下载任务分配给各个子爬虫执行,然后控制不同爬行器之间的数据交换, 协调各个子爬虫的抓取进度。典型代表:M蹦斌一281、UbiCrawer[291、WebFountain[301。
##网络爬虫的研究现状与相关技术简介

###研究现状
目前信息采集系统在国内外都已经出现的较多了,包括各类开源的或者商用的爬虫,它们一般都是广义上的信息采集系统,其突出特点是均面向整个Web范 围来进行研究,可分为如下几类【2】:(1)基于整个Web的信息采集基于整个Web范围的信息采集(ScalableW曲Crawling)是目前应用最广泛的 信息采集方式。从给定若干个种子URL开始扩充,理论上可以逐渐覆盖到整个Web范围。通常大型的门户网站或Web服务提供商会采用该采集方式。典型代表有Google或Mercator。 (2)面向主题的W曲信息采集面向主题的Web信息采集(Focused Web Crawling),简要来说即按照事先设 定的主题,有针对性地采集与主题相关的页面信息。主题的描述一般用关键词, 也可以采用样本文件。相比于与覆盖整个Web范围的信息采集系统,这种采集方 法更有针对性,可以节省可观的硬件和网络资源【31。(3)增量式Web信息采集增量式Web信息采集(Incremental Web Crawling)常用于采集那些内容会时常更新的Web页面。其特点是,在首次信息采集完成之后,若经过一段时间后, 某些采集的信息发生变化或过时,这是后续开始的信息采集工作会有针对性地仅采集那些发生变动的页面,其他没有变化的网页则不予采集。这样的特性使得该 采集方式具有很高的采集效率【4】。典型代表有天网增量搜集系统、Chile Crawler、W.ebFountain和Univ。(4)个性化的Web信息采集个性化的Web信息采集(Customized Web Crawling)主要通过与用户交互等手段,以用户的兴趣为导向,尽可能地满足用户多元化的信息需求。用户的个性 化信息本身的获取可以通过两种方式:用户利用系统提供的接口主动设置或者系 统跟踪用户的使用行为习惯自行学习分析【5】。具有代表性的是SPHINX。(5)分布式Web信息采集分布式Web信息采集(DistributedWeb Crawling)是指通过某种机制的协调,由若干个Web信息采集器负责协同并行采集目标Web集合,相较于其他Web信息 采集方式,在同等条件下,分布式Web信息采集方式往往具备更高的采集速度和 性能【6】。但是如何有效地解决页面负载协调以及共享分布式服务信息这两个问题, 也是该采集方式实现的难点。(6)迁移的信息采集迁移的信息采集(Reloeatable Web Crawling)是将Web采集器上传至目标站 点实现信息采集,通过网络再将采集结果回传到本地。其优点是显而易见的,即 显著地降低了对本地资源的需求。但是运用该方式也存在极大的困难,因为这种 采集方式使目标站点承受病毒攻击的风险,因而需要经过权威机构的评估授权。

###相关技术简介

- Apache Nutch : Nuctch是一个高可扩展，可伸缩的开源网络爬虫项目，由Java语言实现。Nutch作为当今最流行的开源爬虫之一，已经被企业广泛使用。而且Nutch的插件机制使得开发者可以灵活的定制网页抓取策略。另外Nutch有着悠久的发展历史，当今大名鼎鼎的Hadoop就是由Nutch发展而来。Nutch不仅仅可以在单击模式下运行，还可以以分布式的形式运行。


- Heritrix：Heritrix是一个开源的网络爬虫，用户可以用它从网上抓取想要的资源，其最出色之处在于它良好的可扩展性，方便用户实现自己的抓取逻辑。Heritrix主要有三大部件:范围部件，边界部件，处理器链。其中范围部件主要按照规则来决定将哪个URI入队列，边界部件则负责跟踪哪个预定的URI将要被收集，以及已经被收集的URI，选择下一个URI并剔除已经处理过的URI.处理器链包含若干个处理器，负责获取URI，分析结果并将它们传回给边界部件。但Heritrix也有一些局限，比如只在Linux上进行了测试，windows系统暂不支持，每个爬虫都是单独进行工作的，没有对更新做及时的修订工作等。


- Jsoup:Jsoup是一款基于Java的HTML解析器,它向用户提供了非常方便的内容抽取操作接口,可直接从某个URL地址、字符串或文件中解析相应HTML页面内容。Jsoup是基于MIT许可协议发布的,因而可以放心地应用于商业项目。Jsoup有着 和许多Jquery相似的地方,同样提供了强大的select和pipeline的API,在使用方式上也与普通的Javascript有相似之处,便于初学者快速熟悉使用。Jsoup的主要功能包括:
(1)从某个URL、字符串或文件中解析HTML;(2)利用DOM遍历或CSS选择器来查找、提取数据; (3)可操纵HTML元素、属性和文本; (4)可依据一个安全的白名单过滤用户提交的内容,以防止XSS攻击。
通常一个HTML标签元素由标签名、属性和内容这三个部分组成,Jsoup提供 了一套非常简洁的API,可以通过DOM、CSS以及和JQuery类似的选择器语法来 提取和操作数据,这使得Jsoup能够一直保持瘦身。Jsoup的突出优势是:使用起 来十分简单,类似于JavaSeript操作页面DOM对象,对已有JavaScript应用经验 的开发人员来说很直观、熟悉;在标签检索定位方面,Jsoup所提供的功能强大选择器几乎无所不能。


##网络爬虫需要面对的问题


###爬虫中的“黑洞”
在网络爬虫中，有一种被称为“黑洞”的情况，即爬虫在抓取一张网页的链接时，链接本身是一个无限循环。因此，爬虫程序会也会陷入无限循环，从而导致计算资源被白白的浪费而没有产生任何效果。而且，虽然有些URL看起来不同，但是实际上也是指向了同一张网页，这种情况同样会使得爬虫程序陷入重复抓取的境地。
这种现象被称为爬虫“黑洞”。为了防止爬虫陷入“黑洞”，在设计爬虫时，通常都会回避动态网页。识别动态网页时，只需要验证URL中是否存在问号，包含问号的就是动态网页。在线日历就是一种比较容易被忽视的爬虫陷阱，它所生成的动态网页中可以标记上任意日期，并包含指向后一天的额网页链接。一个爬虫从这个日历中爬取到一个网页后，就很容易无止境的请求后一天的网页。从而陷入“黑洞”。

--引用自自己动手写网络爬虫


###爬虫的效率与“礼貌”
如果网络爬虫程序只使用一个单一的线程去执行爬取任务，那么效率将会很低。显而易见的，网络爬虫程序的大量时间都花费在等待响应上了：等待DNS服务器的响应，然后再等待与服务器的连接被确认，之后还要等待从服务器发送出来的网页数据。在等待响应的过程中，网络爬虫程序所在的机器的CPU是处于空闲状态的，而且这时候连网络连接都没有很好的利用起来。为了解决效率的问题，网络爬虫程序使用了多线程，一次运行可以同时抓取数百个网页。然而新的问题又出现了，虽然同时抓取数百个网页对于爬虫程序的开发者或者使用人员来说是一件好事，然而对于网路服务器的拥有者来说却未必是一件好事。想像一下请求队列在实际中的工作场景便可以理解其原因所在。当抓取到一个页面后，比如www.crawler.com,该页面被解析后，页面中所有的超链接都被添加到了请求队列中等待被请求。接下来，爬虫程序会尝试一次抓取所有这些链接指向的页面。如果www.crawler.com所指向的服务器的性能并不是很强大，那么该服务器将需要花费几乎所有的时间和计算资源来处理网络爬虫的请求，从而无法响应真实用户发出的请求。网络爬虫的这种行为会让网络服务器的管理者很生气。

为了避免这个问题，业内的网络爬虫开发者一般会遵循“礼貌策略”。有礼貌的网络爬虫不会再特定的某台网络服务器上一次性抓取多个页面。另外，针对同一个网络服务器的两次请求之间，网络爬虫一般会等待几秒钟，有时可能是几分钟甚至更久。这样就能够使网络服务器将大多数时间和计算资源花费在处理真实用户的请求上。为了支持该策略，请求队列在逻辑上为每个服务器都划分出了一个单独的队列。在任意时间点，这些单独的队列中的绝大多数都会被禁止进行爬取，因为爬虫程序 已经在近期内从相同的服务器上抓取过页面了。而对于那些在礼貌策略内规定的时间窗口内没有被存取的队列，爬虫程序则可以自由地发起页面请求。

-- 引用自《搜索引擎：信息检索实践》

一次抓取几百个网页
###robot.txt
网络爬虫的“礼貌”除了体现在访问频率的限制上，还有另外一种情况。很多网站有许多东西属于自己的“秘密”，本身就不想让网络爬虫抓取，在这种情况下，爬虫程序还是不管不顾的随意抓取，就相当于侵犯了网站的“隐私”。

为了避免这种情况的发生，互联网行业提出了两种解决方案。第一种是在网站的根目录下放置一个文件，起名为robots.txt,其中规定了本站点中哪些内容不希望被爬虫程序抓取。另一种办法则是在网页的源代码中设置Robots Meta标签。其中，放置robots.txt的方式更为常用。

robots.txt文件必须放置在站点的根目录下，而且文件名必须使用小写字母。该文本文件包含一条或多条记录，记录之间用空行分开（用CR,CR/NL或是NL作为结束标志），在该文本文件中可以使用‘#’符合进行注解，具体使用方法和UNIX中的惯例一致。文本文件中的记录通常以一行或多行User-agent开始，后面可以加上若干行Disallow.详细情况如下：

- User-agent:该记录的值用于表示搜索引擎robots的名字，在robots.txt文件中，若有多行User-agent记录，说明有多个robots会受到该协议的限制。对于每个robots.txt文件，至少要有一条User-agent记录。如果想对所有的机器人都设置限制，则只需要设置”User-agent:*”
- Disallow:该记录的值用于指定不希望被机器人访问到的URL,这个URL可以是一条完整的路径，也可以是部分路径。所有以Disallow开头的URL均不会被网络爬虫访问。

##小结

随着人们对个性化信息服务需要的日益增长, 通用搜索引擎表现出越来越多的局限性，定制化的网络爬虫越来越被人们所熟知和应用。本文较为详细的给出了网络爬虫的定义，并从多个维度对其进行了分类阐述和优劣对比。之后还阐述了爬虫技术的研究现状并介绍了一些在应用领域有出色表现的爬虫开发框架。最后，指出了若干网络爬虫技术应用中需要认真思考和应对的问题。




